{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112f0917-01d4-44b2-b9db-027f68b81b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720b4c29-6d39-4e18-909e-b1c98a739c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4165d2-6da6-48e7-9219-90bcc215cc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20\n",
      "Vocab: ['and', 'at', 'barked', 'cat', 'dog', 'fence', 'green', 'in', 'jumped', 'mat', 'on', 'over', 'park', 'quiet', 'ran', 'sat', 'soft', 'the', 'warm', 'was']\n",
      "\n",
      "Tokens: [17, 3, 15, 10, 17, 9, 17, 4, 14, 7]...\n",
      "\n",
      "Training examples: 33\n",
      "Example input: [17, 3, 15] -> 10\n",
      "Words: ['the', 'cat', 'sat'] -> on\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch  20, Loss: 0.0061\n",
      "Epoch  40, Loss: 0.0016\n",
      "Epoch  60, Loss: 0.0008\n",
      "Epoch  80, Loss: 0.0004\n",
      "Epoch 100, Loss: 0.0003\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Generated text:\n",
      "> the cat sat on the mat the dog\n",
      "> the dog ran in the park the cat\n",
      "> the mat was soft and warm the park\n"
     ]
    }
   ],
   "source": [
    "# ============ SIMPLE TEXT DATA ============\n",
    "text = \"\"\"the cat sat on the mat. the dog ran in the park.\n",
    "the cat jumped over the fence. the dog barked at the cat.\n",
    "the mat was soft and warm. the park was green and quiet.\"\"\"\n",
    "\n",
    "# ============ TOKENIZATION ============\n",
    "# Split into words and create vocab\n",
    "words = text.lower().replace(\".\", \"\").split()\n",
    "vocab = sorted(set(words))\n",
    "word_to_id = {w: i for i, w in enumerate(vocab)}\n",
    "id_to_word = {i: w for w, i in word_to_id.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Vocab: {vocab}\\n\")\n",
    "\n",
    "# Convert text to token IDs\n",
    "tokens = [word_to_id[w] for w in words]\n",
    "print(f\"Tokens: {tokens[:10]}...\\n\")\n",
    "\n",
    "# ============ CREATE TRAINING DATA ============\n",
    "# For each token, predict the next token\n",
    "seq_len = 3  # Use 3 tokens to predict the 4th\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(tokens) - seq_len):\n",
    "    X.append(tokens[i:i+seq_len])\n",
    "    y.append(tokens[i+seq_len])\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "print(f\"Training examples: {len(X)}\")\n",
    "print(f\"Example input: {X[0].tolist()} -> {y[0].item()}\")\n",
    "print(f\"Words: {[id_to_word[i.item()] for i in X[0]]} -> {id_to_word[y[0].item()]}\\n\")\n",
    "\n",
    "# ============ SIMPLE LLM MODEL ============\n",
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * 3, hidden_dim)  # 3 tokens flattened\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten: (batch_size, seq_len * embed_dim)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ============ TRAINING ============\n",
    "model = SimpleLLM(vocab_size=len(vocab), embed_dim=16, hidden_dim=32)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(X))\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (len(X) // batch_size)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\\n\")\n",
    "\n",
    "# ============ INFERENCE ============\n",
    "def generate(prompt_words, num_generate=5):\n",
    "    \"\"\"Generate text given a prompt\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert prompt to tokens\n",
    "    current_tokens = [word_to_id[w] for w in prompt_words]\n",
    "    generated = prompt_words.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            # Get last 3 tokens\n",
    "            input_tokens = torch.tensor([current_tokens[-3:]], dtype=torch.long)\n",
    "            logits = model(input_tokens)\n",
    "            \n",
    "            # Pick the token with highest probability\n",
    "            next_token = torch.argmax(logits, dim=1).item()\n",
    "            next_word = id_to_word[next_token]\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            current_tokens.append(next_token)\n",
    "    \n",
    "    return \" \".join(generated)\n",
    "\n",
    "# Test generation\n",
    "print(\"Generated text:\")\n",
    "print(f\"> {generate(['the', 'cat', 'sat'], num_generate=5)}\")\n",
    "print(f\"> {generate(['the', 'dog', 'ran'], num_generate=5)}\")\n",
    "print(f\"> {generate(['the', 'mat', 'was'], num_generate=5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
