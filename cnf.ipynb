{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbb456f-e2a3-4bff-b58e-a9aca2a23cbb",
   "metadata": {},
   "source": [
    "after `pip install --force-reinstall --index-url https://download.pytorch.org/whl/cu118 torch torchvision torchaudio\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44beb29f-3fbe-4810-b80b-014812461f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756fdde9-93bc-4c2e-a993-037ce0efafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0b7bef-6baf-487b-9083-9a2f469901e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f081cbcf7f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53d2578-c3b9-46cf-87ab-5e533011216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Toy dataset: two moons\n",
    "# -----------------------\n",
    "def two_moons(n=2048, noise=0.06, device=\"cpu\"):\n",
    "    n1 = n // 2\n",
    "    n2 = n - n1\n",
    "    # upper moon\n",
    "    t1 = torch.rand(n1, device=device) * math.pi\n",
    "    x1 = torch.stack([torch.cos(t1), torch.sin(t1)], dim=1)\n",
    "    # lower moon (shifted)\n",
    "    t2 = torch.rand(n2, device=device) * math.pi\n",
    "    x2 = torch.stack([1 - torch.cos(t2), -torch.sin(t2) - 0.5], dim=1)\n",
    "    x = torch.cat([x1, x2], dim=0)\n",
    "    x += noise * torch.randn_like(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a510f2-e81f-4297-90cf-f49a6c032e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def standard_normal_logprob(z):\n",
    "    # (B,D) -> (B,)\n",
    "    log_z = -0.5 * math.log(2 * math.pi)\n",
    "    return (log_z - 0.5 * z ** 2).sum(dim=1)\n",
    "\n",
    "def hutchinson_trace(df_dz_v, v):\n",
    "    # df_dz_v is Jv; trace(J) ≈ v^T J v\n",
    "    return (df_dz_v * v).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597d5268-473f-4d45-b68e-2b131aa0704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ODE function f_theta(t, z)\n",
    "# Time is concatenated as a feature (Neural ODE with time conditioning)\n",
    "# -----------------------\n",
    "class ODEfunc(nn.Module):\n",
    "    def __init__(self, dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, dim),\n",
    "        )\n",
    "        # Small weight init helps stability early on\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=0.8)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        # z: (B, D)\n",
    "        t_feat = torch.ones(z.shape[0], 1, device=z.device) * t\n",
    "        inp = torch.cat([z, t_feat], dim=1)\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e52e9f5-1a40-4188-a4a8-805f890e9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# CNF wrapper: integrates (z, logp) jointly\n",
    "# d/dt logp = -Tr(df/dz) (Hutchinson)\n",
    "# -----------------------\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dim, hidden=64, t0=0.0, t1=1.0):\n",
    "        super().__init__()\n",
    "        self.f = ODEfunc(dim, hidden)\n",
    "        self.register_buffer(\"t_span\", torch.tensor([t0, t1], dtype=torch.float32))\n",
    "\n",
    "    def _aug_dynamics(self, t, state):\n",
    "        z, logp, eps = state  # z: (B,D), logp: (B,), eps: (B,D)\n",
    "        z = z.requires_grad_(True)\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            f = self.f(t, z)                             # (B,D)\n",
    "            # Jacobian-vector product J^T * eps  (or J * eps—either works for trace with same eps)\n",
    "            # We compute Jv by autograd: grad(f, z, v) gives v^T J (i.e., J^T v), so use same eps\n",
    "            Jt_eps = grad(f, z, eps, retain_graph=True, create_graph=True)[0]  # (B,D)\n",
    "            # trace(J) ≈ v^T J v == (J^T v)·v\n",
    "            trace_est = hutchinson_trace(Jt_eps, eps)     # (B,)\n",
    "            dlogp = -trace_est\n",
    "        return (f, dlogp, torch.zeros_like(eps))          # eps is constant in time\n",
    "\n",
    "    def transform(self, x, reverse=False):\n",
    "        \"\"\"\n",
    "        If reverse=False: x -> z (to base) integrating t0->t1\n",
    "        If reverse=True:  z -> x (for sampling) integrating t1->t0\n",
    "        Returns (z_or_x, logp_delta)\n",
    "        \"\"\"\n",
    "        B, D = x.shape\n",
    "        device = x.device\n",
    "        # Rademacher noise usually works well; Gaussian also fine.\n",
    "        eps = torch.randint(0, 2, x.shape, device=device, dtype=torch.float32) * 2 - 1\n",
    "\n",
    "        if reverse:\n",
    "            t_span = torch.flip(self.t_span, dims=[0])\n",
    "        else:\n",
    "            t_span = self.t_span\n",
    "\n",
    "        logp0 = torch.zeros(B, device=device)\n",
    "        state0 = (x, logp0, eps)\n",
    "        zT, logp_T, _ = odeint(self._aug_dynamics, state0, t_span, atol=1e-5, rtol=1e-5)\n",
    "        z1 = zT[-1]\n",
    "        logp1 = logp_T[-1]\n",
    "        return z1, logp1\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        z, delta_logp = self.transform(x, reverse=False)\n",
    "        logpz = standard_normal_logprob(z)\n",
    "        return logpz + delta_logp  # log p_x(x) = log p_z(z) + ∫ dlogp\n",
    "\n",
    "    def sample(self, n, device=\"cpu\"):\n",
    "        z = torch.randn(n, 2, device=device)\n",
    "        x, _ = self.transform(z, reverse=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec907c9-8ce0-4283-bdc3-96f803ec519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Train loop\n",
    "# -----------------------\n",
    "def train(device=\"cpu\"):\n",
    "    dim = 2\n",
    "    model = CNF(dim=dim, hidden=64, t0=0.0, t1=1.0).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    batch_size = 512\n",
    "\n",
    "    for step in range(4000):\n",
    "        x = two_moons(n=batch_size, device=device)\n",
    "        # Maximize log-likelihood (minimize NLL)\n",
    "        logpx = model.log_prob(x)\n",
    "        loss = -logpx.mean()\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        # Optional: gradient clipping for stability\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        opt.step()\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            print(f\"step {step+1:4d}  NLL: {loss.item():.3f}\")\n",
    "\n",
    "    # Sample a few points after training\n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(4096, device=device).cpu()\n",
    "    return model, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463dcd5-1554-4aed-a813-e761cb35036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model, samples = train(device=device)\n",
    "# Save samples if you want to plot later\n",
    "torch.save(samples, \"cnf_samples.pt\")\n",
    "print(\"Saved samples to cnf_samples.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
