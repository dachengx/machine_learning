{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a74cbd-3d3d-4d4a-9684-58fb5a74a96e",
   "metadata": {},
   "source": [
    "Text–Time Series Fusion Model (TTSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ede3689-627a-41db-8260-61b227293d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fa6b0-0878-4194-b27c-f790d39c0895",
   "metadata": {},
   "source": [
    "# Build sentiment features with an LLM encoder (FinBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff479c31-3886-4021-a40f-d8760643901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 15:34:53.064404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-22 15:34:53.951957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Scoring headlines: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "prices = pd.read_csv(\"data/prices.csv\", parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "news   = pd.read_csv(\"data/news.csv\",   parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "# Choose a finance-sentiment model (both are widely used)\n",
    "MODEL_NAME = \"ProsusAI/finbert\"  # alt: \"yiyanghkust/finbert-tone\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "def batch_sentiment(texts, batch_size=16, max_length=128):\n",
    "    \"\"\"Return list of dicts with probs: {'neg': p0, 'neu': p1, 'pos': p2} (FinBERT label order).\"\"\"\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Scoring headlines\"):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            tok = tokenizer(chunk, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            logits = model(**{k: v.to(\"cuda:0\") for k, v in tok.items()}).logits\n",
    "            probs = softmax(logits, dim=-1).cpu().numpy()\n",
    "            for p in probs:\n",
    "                out.append({\"neg\": float(p[0]), \"neu\": float(p[1]), \"pos\": float(p[2])})\n",
    "    return out\n",
    "\n",
    "# Compute sentiment per headline\n",
    "news = news.dropna(subset=[\"headline\"]).copy()\n",
    "scores = batch_sentiment(news[\"headline\"].astype(str).tolist(), batch_size=32)\n",
    "news = pd.concat([news.reset_index(drop=True), pd.DataFrame(scores)], axis=1)\n",
    "\n",
    "# Aggregate to daily features (you can try other aggregations)\n",
    "daily_sent = (\n",
    "    news.groupby(\"date\")\n",
    "        .agg(pos_mean=(\"pos\", \"mean\"), neg_mean=(\"neg\", \"mean\"), neu_mean=(\"neu\", \"mean\"),\n",
    "             pos_max=(\"pos\", \"max\"),  neg_max=(\"neg\", \"max\"),\n",
    "             n_headlines=(\"headline\", \"count\"))\n",
    "        .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7635eb-bafc-4647-be69-3b0ad20c7009",
   "metadata": {},
   "source": [
    "# Merge with price features and make supervised sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e71436f-ccb3-4e5f-bbef-58f6de43b46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 10, 9), (12, 10, 9), (12, 10, 9))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prices.merge(daily_sent, on=\"date\", how=\"left\").fillna(0.0)\n",
    "\n",
    "# Simple price features (extend with TA if you like)\n",
    "df[\"ret1\"] = df[\"close\"].pct_change()\n",
    "df[\"hl_spread\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"].shift(1)\n",
    "df[\"vol_norm\"]  = (df[\"volume\"] / df[\"volume\"].rolling(20).mean()).fillna(1.0)\n",
    "\n",
    "# Target: next-day return (regression) or sign (classification)\n",
    "df[\"y_reg\"] = df[\"ret1\"].shift(-1)\n",
    "df[\"y_cls\"] = (df[\"y_reg\"] > 0).astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"ret1\", \"hl_spread\", \"vol_norm\",\n",
    "    \"pos_mean\", \"neg_mean\", \"neu_mean\", \"pos_max\", \"neg_max\", \"n_headlines\",\n",
    "]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time (no leakage!)\n",
    "split1 = int(len(df) * 0.7)\n",
    "split2 = int(len(df) * 0.85)\n",
    "train, val, test = df.iloc[:split1], df.iloc[split1:split2], df.iloc[split2:]\n",
    "\n",
    "# Standardize using *train* statistics only\n",
    "scaler = StandardScaler().fit(train[feature_cols])\n",
    "def scale_block(block):\n",
    "    X = scaler.transform(block[feature_cols])\n",
    "    y_reg = block[\"y_reg\"].values\n",
    "    y_cls = block[\"y_cls\"].values\n",
    "    return X, y_reg, y_cls\n",
    "\n",
    "Xtr, ytr_reg, ytr_cls = scale_block(train)\n",
    "Xva, yva_reg, yva_cls = scale_block(val)\n",
    "Xte, yte_reg, yte_cls = scale_block(test)\n",
    "\n",
    "# Build sliding windows for LSTM\n",
    "def make_windows(X, y_reg, y_cls, window=60, horizon=1, task=\"reg\"):\n",
    "    Xw, Y = [], []\n",
    "    for i in range(len(X) - window - horizon + 1):\n",
    "        Xw.append(X[i:i + window])\n",
    "        if task == \"reg\":\n",
    "            Y.append(y_reg[i+window+horizon-1])\n",
    "        else:\n",
    "            Y.append(y_cls[i+window+horizon-1])\n",
    "    return np.array(Xw, dtype=np.float32), np.array(Y)\n",
    "\n",
    "WINDOW, H = 10, 1\n",
    "Xtr_w, Ytr = make_windows(Xtr, ytr_reg, ytr_cls, WINDOW, H, task=\"reg\")\n",
    "Xva_w, Yva = make_windows(Xva, yva_reg, yva_cls, WINDOW, H, task=\"reg\")\n",
    "Xte_w, Yte = make_windows(Xte, yte_reg, yte_cls, WINDOW, H, task=\"reg\")\n",
    "\n",
    "Xtr_w.shape, Xva_w.shape, Xte_w.shape  # (N, window, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5dfd-293c-4601-87d3-0596d367f1d3",
   "metadata": {},
   "source": [
    "# LSTM regressor over fused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf90a8f1-45ee-4a17-ac03-df004d35a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | train 0.021497 | val 0.007390\n",
      "Epoch 001 | train 0.004815 | val 0.000195\n",
      "Epoch 002 | train 0.001796 | val 0.002263\n",
      "Epoch 003 | train 0.006059 | val 0.002409\n",
      "Epoch 004 | train 0.006727 | val 0.000714\n",
      "Epoch 005 | train 0.003787 | val 0.000117\n",
      "Epoch 006 | train 0.001646 | val 0.002009\n",
      "Epoch 007 | train 0.000593 | val 0.005616\n",
      "Epoch 008 | train 0.001526 | val 0.008409\n",
      "Epoch 009 | train 0.002610 | val 0.008909\n",
      "Epoch 010 | train 0.003042 | val 0.007072\n",
      "Epoch 011 | train 0.002307 | val 0.004243\n",
      "Epoch 012 | train 0.001661 | val 0.001720\n",
      "Epoch 013 | train 0.000748 | val 0.000331\n",
      "Test MSE: 0.0002947589  | Directional accuracy: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(Xtr_w, dtype=torch.float32), torch.tensor(Ytr, dtype=torch.float32).unsqueeze(-1))\n",
    "val_ds   = TensorDataset(torch.tensor(Xva_w, dtype=torch.float32), torch.tensor(Yva, dtype=torch.float32).unsqueeze(-1))\n",
    "test_ds  = TensorDataset(torch.tensor(Xte_w, dtype=torch.float32), torch.tensor(Yte, dtype=torch.float32).unsqueeze(-1))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256)\n",
    "\n",
    "class LSTMReg(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64, layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features, hidden_size=hidden, num_layers=layers,\n",
    "            dropout=dropout if layers > 1 else 0.0, batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)\n",
    "        # use last timestep\n",
    "        return self.head(out[:, -1, :])\n",
    "\n",
    "model = LSTMReg(n_features=Xtr_w.shape[-1]).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience = 8\n",
    "bad = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tr_loss += loss.item() * len(xb)\n",
    "    tr_loss /= len(train_ds)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va_loss = 0.0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            va_loss += loss_fn(pred, yb).item() * len(xb)\n",
    "        va_loss /= len(val_ds)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train {tr_loss:.6f} | val {va_loss:.6f}\")\n",
    "    if va_loss < best_val - 1e-5:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict({k:v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "# Evaluate (MSE and directional accuracy)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds, ys = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds.append(model(xb).cpu().numpy())\n",
    "        ys.append(yb.numpy())\n",
    "\n",
    "preds = np.concatenate(preds).ravel()\n",
    "ys    = np.concatenate(ys).ravel()\n",
    "mse = np.mean((preds - ys) ** 2)\n",
    "direction_acc = np.mean((preds > 0) == (ys > 0))\n",
    "print(\"Test MSE:\", mse, \" | Directional accuracy:\", direction_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
